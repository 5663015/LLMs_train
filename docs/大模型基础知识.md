# 大模型基础

## 语言模型

**语言模型定义：**给定一句由 $n$ 个单词组成的句子 $W=w_1, w_2, \dots, w_n$，计算这个句子的概率 $P(w_1, w_2, \dots, w_n)$，或者根据前文计算下一个词的概率 $P(w_n | w_1, w_2, \dots, w_{n-1})$ 的模型。

**神经网络语言模型：**训练数据是一个单词序列 $w_1, w_2, \dots, w_T, w_t \in V$，其中字典 $V$ 是一个有限集合。模型的目标是学习一个语言模型：
$$
f(w_t, \dots, w_{t-n+1})= \hat{P} (w_t | w_1^{t-1})
$$
其中，$w_t$ 表示单词序列中第 $t$ 个单词，$w_t, \dots, w_{t-n+1}$ 表示由 $n$ 个单词组成的子序列。整个神经网络语言模型是一个三层的神经网络，最下面的输入即前 $n-1$ 个单词 $w_{t-n+1}, \dots, w_{t-2}, w_{t-1}$，希望通过这 $n-1$ 个单词来预测下一个单词 $w_t$。

## Transformer

目前大模型的基础神经网络架构是Transformer，其基本结构如下图所示，包含N个编码器和N个解码器。

<img src="../assets/transformer.jpg" style="zoom:50%;" />

每一个编码器中包含两个子网络，即：一个多注意力头 Multi-Head Attention 和一个前馈神经网络。每个子网络都具有残差连接。











## ChatGPT训练过程





## LoRA





## 现有主要大模型列表





## 参考







